{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,precision_recall_curve, roc_auc_score, f1_score, fbeta_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('clas_data.xlsx').drop('Unnamed: 0', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.21 ms, sys: 114 µs, total: 5.33 ms\n",
      "Wall time: 5.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['clas'])\n",
    "data['clas'] = le.transform(data['clas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.07 ms, sys: 102 µs, total: 5.18 ms\n",
      "Wall time: 5.02 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['summ_lemma'], data['clas'], test_size=0.12, stratify=data['clas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23954 unique tokens.\n",
      "(23955, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "VOCAB_SIZE = 80000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data['summ_lemma'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "from navec import Navec\n",
    "\n",
    "path = 'navec_news_v1_1B_250K_300d_100q.tar'\n",
    "word2vec = Navec.load(path)\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels_train = to_categorical(np.asarray(y_train), num_classes=len(le.classes_), dtype='float64')\n",
    "labels_test = to_categorical(np.asarray(y_test), num_classes=len(le.classes_), dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #https://arxiv.org/abs/1408.5882\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs,axis=1)\n",
    "\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu',kernel_regularizer='l1_l2')(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     7186500     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 198, 128)     115328      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 197, 128)     153728      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 196, 128)     192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 66, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 65, 128)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 65, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 196, 128)     0           max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 196, 128)     0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 25088)        0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          3211392     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            1032        dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,860,108\n",
      "Trainable params: 3,673,608\n",
      "Non-trainable params: 7,186,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH,\n",
    "                len(train_word_index)+1, EMBEDDING_DIM, len(list(le.classes_)), False)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.02, patience=4, verbose=1,mode = 'max')\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "37/37 [==============================] - 16s 419ms/step - loss: 0.3630 - categorical_accuracy: 0.3981 - precision_2: 0.3928 - recall_2: 0.2334 - val_loss: 0.1803 - val_categorical_accuracy: 0.7149 - val_precision_2: 0.8271 - val_recall_2: 0.5584\n",
      "Epoch 2/25\n",
      "37/37 [==============================] - 15s 405ms/step - loss: 0.1539 - categorical_accuracy: 0.7753 - precision_2: 0.8514 - recall_2: 0.6379 - val_loss: 0.1421 - val_categorical_accuracy: 0.7863 - val_precision_2: 0.8470 - val_recall_2: 0.6990\n",
      "Epoch 3/25\n",
      "37/37 [==============================] - 15s 405ms/step - loss: 0.1072 - categorical_accuracy: 0.8490 - precision_2: 0.8904 - recall_2: 0.7651 - val_loss: 0.1192 - val_categorical_accuracy: 0.8149 - val_precision_2: 0.8667 - val_recall_2: 0.7593\n",
      "Epoch 4/25\n",
      "37/37 [==============================] - 15s 406ms/step - loss: 0.0818 - categorical_accuracy: 0.8892 - precision_2: 0.9134 - recall_2: 0.8361 - val_loss: 0.1132 - val_categorical_accuracy: 0.8237 - val_precision_2: 0.8591 - val_recall_2: 0.7895\n",
      "Epoch 5/25\n",
      "37/37 [==============================] - 15s 405ms/step - loss: 0.0576 - categorical_accuracy: 0.9308 - precision_2: 0.9370 - recall_2: 0.8884 - val_loss: 0.1120 - val_categorical_accuracy: 0.8316 - val_precision_2: 0.8883 - val_recall_2: 0.7705\n",
      "Epoch 6/25\n",
      "37/37 [==============================] - 15s 412ms/step - loss: 0.0434 - categorical_accuracy: 0.9590 - precision_2: 0.9625 - recall_2: 0.9254 - val_loss: 0.1193 - val_categorical_accuracy: 0.8300 - val_precision_2: 0.8646 - val_recall_2: 0.7911\n",
      "Epoch 7/25\n",
      "37/37 [==============================] - 15s 406ms/step - loss: 0.0367 - categorical_accuracy: 0.9667 - precision_2: 0.9704 - recall_2: 0.9356 - val_loss: 0.1266 - val_categorical_accuracy: 0.8261 - val_precision_2: 0.8632 - val_recall_2: 0.7768\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_cnn_data, labels_train,\n",
    "                 callbacks=callbacks_list, \n",
    "                 validation_data=(test_cnn_data, labels_test),\n",
    "                 epochs=25, \n",
    "                 batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1470386e-05 9.9797523e-01 2.0718575e-04 ... 3.6019087e-04\n",
      "  3.7425756e-04 7.6007843e-04]\n",
      " [7.8430772e-04 9.9759412e-01 3.5792589e-04 ... 6.1092719e-06\n",
      "  3.9671664e-05 4.1635036e-03]\n",
      " [1.1403396e-05 1.9850924e-05 6.8085404e-09 ... 9.9995244e-01\n",
      "  1.1116117e-06 2.5986486e-07]\n",
      " ...\n",
      " [4.2509735e-03 1.2557188e-05 2.6159923e-05 ... 9.4246244e-01\n",
      "  3.2940507e-04 1.1045306e-04]\n",
      " [7.2537121e-05 6.4400128e-06 9.9994600e-01 ... 4.0625791e-06\n",
      "  1.8525124e-04 6.4400039e-05]\n",
      " [6.1796945e-06 1.1671710e-04 2.2435084e-05 ... 9.9895710e-01\n",
      "  8.2483748e-05 1.7181039e-04]]\n"
     ]
    }
   ],
   "source": [
    "model_w2v_pred = model.predict(test_cnn_data)\n",
    "print(model_w2v_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363780778395552\n",
      "Precision: 0.8334531286613687\n",
      "Recall: 0.8384656603902109\n",
      "F1: 0.8345254344732432\n",
      "F0.5: 0.8335448137770236\n",
      "F2: 0.8365361903084823\n"
     ]
    }
   ],
   "source": [
    "cnn_nav_test = []\n",
    "for i in model_w2v_pred:\n",
    "    cnn_nav_test.append(np.argmax(i))\n",
    "print('Accuracy:', accuracy_score(y_test, cnn_nav_test))\n",
    "print('Precision:', precision_score(y_test, cnn_nav_test,average='macro'))           \n",
    "print('Recall:', recall_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F1:', f1_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F0.5:', fbeta_score(y_test, cnn_nav_test, beta = 0.5,average='macro'))\n",
    "print('F2:', fbeta_score(y_test, cnn_nav_test, beta = 2,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8260524225575854\n",
      "Precision: 0.8398643066774344\n",
      "Recall: 0.8180115897250012\n",
      "F1: 0.8256922956764837\n",
      "F0.5: 0.8334146951156821\n",
      "F2: 0.8202928800857534\n"
     ]
    }
   ],
   "source": [
    "cnn_nav_test = []\n",
    "for i in model_w2v_pred:\n",
    "    cnn_nav_test.append(np.argmax(i))\n",
    "print('Accuracy:', accuracy_score(y_test, cnn_nav_test))\n",
    "print('Precision:', precision_score(y_test, cnn_nav_test,average='macro'))           \n",
    "print('Recall:', recall_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F1:', f1_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F0.5:', fbeta_score(y_test, cnn_nav_test, beta = 0.5,average='macro'))\n",
    "print('F2:', fbeta_score(y_test, cnn_nav_test, beta = 2,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8196981731532963\n",
      "Precision: 0.8212842556445457\n",
      "Recall: 0.8118389313759845\n",
      "F1: 0.8158182983415764\n",
      "F0.5: 0.8189132235869094\n",
      "F2: 0.8132578400970272\n"
     ]
    }
   ],
   "source": [
    "cnn_nav_test = []\n",
    "for i in model_w2v_pred:\n",
    "    cnn_nav_test.append(np.argmax(i))\n",
    "print('Accuracy:', accuracy_score(y_test, cnn_nav_test))\n",
    "print('Precision:', precision_score(y_test, cnn_nav_test,average='macro'))           \n",
    "print('Recall:', recall_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F1:', f1_score(y_test, cnn_nav_test,average='macro'))\n",
    "print('F0.5:', fbeta_score(y_test, cnn_nav_test, beta = 0.5,average='macro'))\n",
    "print('F2:', fbeta_score(y_test, cnn_nav_test, beta = 2,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_lex_150.h5') #108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_lex_150.h5') #108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_nn_250.h5') #96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('cnn_pgn_150_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
